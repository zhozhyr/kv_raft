# Raft-based Distributed Key-Value Store

Учебный проект по реализации распределённого отказоустойчивого **Key-Value хранилища** с использованием алгоритма консенсуса **Raft**.

Проект выполнен в рамках курсовой работы и демонстрирует:
- выбор лидера,
- репликацию журнала,
- достижение консенсуса большинством узлов,
- устойчивость к отказу отдельных узлов кластера.

---

## Об архитектуре

Система представляет собой кластер из нескольких узлов (по умолчанию — 3), где каждый узел:

- является полноценным Raft-узлом (follower / candidate / leader),
- хранит:
  - журнал операций (log),
  - текущее состояние Key-Value хранилища,
- обменивается RPC-запросами с другими узлами по HTTP.

### Используемые технологии
- **Python 3.13**
- **FastAPI**
- **Raft** — алгоритм консенсуса
- **Docker / Docker Compose** — развёртывание кластера
- **pytest + httpx** — интеграционные тесты

---

## Реализованные возможности

### Raft
- Выбор лидера (Leader Election)
- Heartbeat’ы
- Репликация лога
- Коммит записей только после достижения **quorum**
- Безопасность: коммит только записей текущего term
- Перевыбор лидера при его отказе

### Key-Value API
- `PUT /kv/{key}` — запись значения (через лидера)
- `GET /kv/{key}` — чтение значения
- Данные реплицируются между узлами

### Надёжность
- Кластер продолжает работать при падении одного узла из трёх
- Данные не теряются при перевыборе лидера
- Узлы могут перезапускаться и восстанавливать состояние из persistent storage

---

## HTTP API

### Клиентские запросы
| Метод    | Endpoint | Описание                         |
|----------|--------|----------------------------------|
| `PUT`    | `/kv/{key}` | Записать значение (через лидера) |
| `GET`    | `/kv/{key}` | Получить значение                |
| `DELETE` | `/kv/{key}` | Удалить значение                 |

### Raft RPC (внутренние)
| Метод | Endpoint |
|-----|---------|
| `POST` | `/raft/request_vote` |
| `POST` | `/raft/append_entries` |

### Отладка
| Метод | Endpoint |
|-----|---------|
| `GET` | `/state` |

---

## Тестирование

Проект покрыт **интеграционными тестами**, которые проверяют:

- наличие ровно одного лидера в кластере,
- запись и чтение данных,
- сохранность данных после падения лидера,
- перевыбор лидера и продолжение работы кластера.

Тесты написаны с использованием `pytest` и запускаются поверх реального Docker-кластера.

---

## Запуск проекта

### 1. Сборка и запуск кластера
```bash
docker compose up --build
